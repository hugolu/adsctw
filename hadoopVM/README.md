# æ‰“é€ è‡ªå·±çš„ Hadoop è™›æ“¬æ©Ÿå™¨

## å‹•æ©Ÿ

ä¹‹å‰å¾ Hortonworks ä¸‹è¼‰ [hortonworks sandbox](http://hortonworks.com/products/hortonworks-sandbox/#install) VM å½±åƒæª”ï¼Œè©¦ç©ç™¼ç¾è£¡é¢åŸ·è¡Œå¤ªå¤šæœå‹™ï¼Œä¸ä½†è‚¥å¤§ä¹Ÿå¾ˆè€—è³‡æºï¼Œæ±ºå®šæŠŠä¹‹å‰å®‰è£ ubuntu/trusty64 è™›æ“¬æ©Ÿã€æ¶è¨­Hadoopç›¸é—œæœå‹™çš„éç¨‹å¯«æˆç­†è¨˜ã€‚

é€™ç¯‡æ–‡ç« æ²’æœ‰åŒ…å±±åŒ…æµ·çš„ä¼åœ–ï¼Œä¹Ÿæ²’æœ‰æƒ³è¦çœŸæ­£æ¶è¨­ä¸€å€‹Hadoop Clusterï¼Œåªæƒ³å–®å­˜å¼„ä¸€å€‹ç°¡æ˜“çš„å–®æ©Ÿå½åˆ†ä½ˆå¼ç³»çµ±(Pseudo-Distributed Mode)ï¼Œå¯ä»¥è®“è‡ªå·±åœ¨ä¸Šé¢è¤‡ç¿’èª²å ‚ä¸Šé™¤äº†Azure Machine Learningã€HDinsightä¹‹å¤–çš„èª²ç¨‹ã€‚è¦å»ºç½®é€™æ¨£çš„ç’°å¢ƒï¼Œéœ€è¦çš„æœå‹™åŒ…å«ï¼š
- Hadoop HDFS/MapReduce
- Hive
- MySQL
- Sqoop
- Scala
- Spark

## éœ€æ±‚

- å®‰è£ [VirtualBox](https://www.virtualbox.org/wiki/Downloads)
- å®‰è£ [Vagrant](https://www.vagrantup.com/downloads.html)
- è¨˜æ†¶é«” 2G (è¦è·‘Spark)
- ç¡¬ç¢Ÿç©ºé–“ 10G (ä»¥æˆ‘çš„caseï¼Œ10Gå°±å¤ ç”¨)

## å®‰è£ã€è¨­å®šã€é€£æ¥ VM

ä»¥ä¸‹å‹•ä½œåœ¨æˆ‘çš„ MacBookPro ä½¿ç”¨ iTerm æ“ä½œï¼Œåªåˆ—å‡ºç›¸é—œæŒ‡ä»¤ï¼Œä¸é¡¯ç¤ºå®‰è£éç¨‹çµ‚ç«¯æ©Ÿè¼¸å‡ºçš„è¨Šæ¯ã€‚

å‰µå»ºadsctwç›®éŒ„ï¼ŒæŠŠVMå®‰è£åœ¨æ­¤
```shell
$ mkdir adsctw
$ cd adsctw
```

ä¸‹è¼‰vagrant box
```shell
$ vagrant init ubuntu/trusty64
```

å•Ÿå‹•å‰ä¿®æ”¹VMè¨˜æ†¶é«”é…ç½®
```shell
$ vi Vagrantfile
```
```
  config.vm.provider "virtualbox" do |vb|
    vb.memory = "2048"
  end
```

å•Ÿå‹•VMï¼Œå¦‚æœä½ æœ‰å®‰è£å¤šå€‹è™›æ“¬æ©Ÿè»Ÿé«”ï¼Œè«‹åŠ ä¸Š```--provider virtualbox```é¸é …æŒ‡å®švirtualbox
```shell
$ vagrant up
```

å•Ÿå‹•å¾Œä½¿ç”¨sshé€£ç·šç™»å…¥ï¼Œç™»å…¥è€…åç¨±ç‚º **vagrant**
```shell
$ vagrant ssh
vagrant@vagrant-ubuntu-trusty-64:~$
```

## APT

å®‰è£å¥—ä»¶æœ‰å…©ç¨®æ–¹å¼ï¼Œä¸€å€‹æ˜¯ä¸‹è¼‰sourceè‡ªè¡Œç·¨è­¯ã€å¦ä¸€å€‹æ˜¯ä½¿ç”¨å¥—ä»¶ç®¡ç†å·¥å…·ï¼Œé™¤éä½ æƒ³ä¿®æ”¹(hack)å¥—ä»¶å…§å®¹ä¸ç„¶ä¸€èˆ¬æœƒé¸æ“‡ä½¿ç”¨ç®¡ç†å¥—ä»¶å·¥å…·ã€‚ç‚ºäº†è®“è‡ªå·±ç”Ÿæ´»æ„‰å¿«ï¼Œæˆ‘åœ¨é€™ä»½èªªæ˜æ–‡ä»¶ä¸­ä½¿ç”¨[APT](https://en.wikipedia.org/wiki/Advanced_Packaging_Tool)å®‰è£å·¥å…·èˆ‡æœå‹™ç¨‹å¼ã€‚

æ›´æ–°apt source listï¼Œä¸¦å®‰è£å¾…éœ€è¦ç”¨åˆ°çš„å·¥å…·ã€‚
```shell
$ sudo apt-get update
$ sudo apt-get install -y git unzip tree
```

## User

å‰µå»ºæœ‰åŸ·è¡ŒHadoop/HDFSæ¬Šé™çš„ä½¿ç”¨è€…**hadoop**ï¼ŒæŠŠå®ƒåŠ å…¥**sudo**ç¾¤çµ„ï¼Œé€™æ¨£å°±èƒ½ä½¿ç”¨**root**æ¬Šé™åŸ·è¡ŒæŒ‡ä»¤ã€‚
```shell
$ sudo useradd -m hadoop -s /bin/bash
$ sudo passwd hadoop
$ sudo adduser hadoop sudo
```

OKï¼æ¥ä¸‹ä¾†æ‰€æœ‰å‹•ä½œæ›æˆ**hadoop**èº«ä»½ä¾†æ“ä½œã€‚
```shell
$ sudo su - hadoop
```

Hadoopä½¿ç”¨sshå­˜å–é ç«¯ï¼Œå³ä½¿æ˜¯å–®æ©Ÿä¹Ÿéœ€è¦é€£æ¥localhostã€‚æ¥ä¸‹ä¾†ï¼Œèªè­‰è‡ªå·±çš„ssh public keyï¼Œä»¥ä¾¿å°‡ä¾†å¯ä»¥ä¸éœ€ä½¿ç”¨å¯†ç¢¼ç™»å…¥ã€‚
```shell
$ ssh-keygen -t rsa
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

æ¸¬è©¦ä¸€ä¸‹ï¼Œæ­è€¶ï½ç¢ºå®šç™»å…¥ä¸ç”¨å¯†ç¢¼å¾Œé›¢é–‹
```
$ ssh localhost
$ exit
```

## Java SDK

å®‰è£Ubuntuè‡ªå¸¶çš„openjdk-7-jdk
```shell
$ sudo apt-get install -y openjdk-7-jdk
```

è¨­å®šç’°å¢ƒè®Šæ•¸ï¼Œä¿®æ”¹```~/.bashrc``` (ps.ä»¥ä¸‹æœ‰é—œæ–‡ä»¶ä¿®æ”¹çš†ä½¿ç”¨vimæ“ä½œ)
```shell
$ vim ~/.bashrc
```

åŠ å…¥ä»¥ä¸‹å…§å®¹
```
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
```

æª¢æŸ¥å®‰è£çš„ç‰ˆæœ¬
```shell
$ java -version
```

## Hadoop/HDFS

åƒè€ƒè³‡æ–™
- [Hadoopå®‰è£…é…ç½®ç®€ç•¥æ•™ç¨‹](http://www.powerxing.com/install-hadoop-simplify/)
- [Running Hadoop on Ubuntu Linux (Single-Node Cluster)](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/)
- [Hadoop: Setting up a Single Node Cluster](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html)

åˆ°[Apache Download Mirrors](http://www.apache.org/dyn/closer.cgi/hadoop/core)ä¸‹è¼‰æœ€æ–°çš„hadoop packageï¼Œè§£å£“ç¸®å¾Œæ¬ç§»åˆ°```/usr/local```ç›®éŒ„ä¹‹ä¸‹ã€‚
```shell
$ wget http://ftp.tc.edu.tw/pub/Apache/hadoop/core/hadoop-2.7.1/hadoop-2.7.1.tar.gz
$ tar zxf hadoop-2.7.1.tar.gz
$ sudo mv hadoop-2.7.1 /usr/local/hadoop
```

è¨­å®šç’°å¢ƒè®Šæ•¸ï¼Œä¿®æ”¹```~/.bashrc```åŠ å…¥ä»¥ä¸‹å…§å®¹
```
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export CLASSPATH=$CLASSPATH:$(hadoop classpath)
```

è¨­å®š hadoop core-siteï¼Œä¿®æ”¹```/usr/local/hadoop/etc/hadoop/core-site.xml```å…§å®¹
```
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

è¨­å®š hadoop hdfs-siteï¼Œä¿®æ”¹```/usr/local/hadoop/etc/hadoop/hdfs-site.xml```å…§å®¹
```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/data</value>
    </property>
</configuration>
```

è¨­å®šhadoopç’°å¢ƒè®Šæ•¸ï¼Œä¿®æ”¹```/usr/local/hadoop/etc/hadoop/hadoop-env.sh```å…§å®¹
```
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
```

æ ¼å¼åŒ–HDFSæª”æ¡ˆç³»çµ±
```shell
$ hdfs namenode -format
```

å•Ÿå‹•HDFSæœå‹™ï¼Œä¸¦æª¢è¦–é‚£äº›æœå‹™è¢«å•Ÿå‹•
```shell
$ start-dfs.sh
$ jps
```

æ¸¬è©¦ HDFS/MapReduce åŠŸèƒ½æ˜¯å¦æ­£å¸¸
```shell
$ hadoop fs -mkdir -p /user/hadoop
$ hadoop fs -mkdir input
$ hadoop fs -put /usr/local/hadoop/etc/hadoop/*.xml input
$ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep input output 'dfs[a-z.]+'
$ hadoop fs -cat output/*
```

## Hive

åƒè€ƒè³‡æ–™
- [Apache Hive Getting Started](https://cwiki.apache.org/confluence/display/Hive/GettingStarted)

åˆ°[Apache Download Mirrors](http://www.apache.org/dyn/closer.cgi/hive/)ä¸‹è¼‰æœ€æ–°çš„hive packageï¼Œè§£å£“ç¸®å¾Œæ¬ç§»åˆ°```/usr/local```ç›®éŒ„ä¹‹ä¸‹ã€‚
```shell
$ wget http://ftp.tc.edu.tw/pub/Apache/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz
$ tar zxf apache-hive-1.2.1-bin.tar.gz -C /usr/local/
$ sudo mv apache-hive-1.2.1-bin /usr/local/hive
```

è¨­å®šç’°å¢ƒè®Šæ•¸ï¼Œä¿®æ”¹```~/.bashrc```åŠ å…¥ä»¥ä¸‹å…§å®¹
```
export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
```

æ¸¬è©¦ Hive åŠŸèƒ½æ˜¯å¦æ­£å¸¸
```shell
$ hive -e "select 1+1"
```

## MySQL

Hadoopè³‡æ–™å¯èƒ½ä¾†è‡ªå‚³çµ±çµæ§‹åŒ–è³‡æ–™åº«ï¼Œç¬¬ä¸€å¤©æœ‰èª²ç¨‹æ•™å°å¦‚ä½•æŠŠç´€éŒ„å¾MySQLé·ç§»åˆ°Hiveä¸Šã€‚Soï¼Œé †ä¾¿å®‰è£MySQLã€‚

å®‰è£Ubuntuè‡ªå¸¶çš„mysql-serverï¼Œä½¿ç”¨ä¸‹é¢æŒ‡ä»¤æœƒä¸€ä½µå®‰è£mysql-serverã€mysql-clientã€mysql-commonã€‚å®‰è£éç¨‹æœƒè©¢å•**root**å¯†ç¢¼ï¼Œéš¨ä¾¿æ‰“ä¸€å€‹æ–¹ä¾¿è¨˜æ†¶çš„å­—ä¸²å³å¯ã€‚
```shell
$ sudo apt-get install -y mysql-server
```

å‰µå»ºä¸€å€‹æ“ä½œè³‡æ–™åº«çš„ä½¿ç”¨è€…ï¼Œèª²ç¨‹é è¨­å¸³è™Ÿ/å¯†ç¢¼ç‚º**hive**/**hive**ã€‚å…ˆä½¿ç”¨**root**èº«ä»½ç™»å…¥ç™»å…¥mysql shellï¼Œçœ‹åˆ°æç¤ºç¬¦è™Ÿç”±```$```è®Šæˆ```mysql>```ï¼Œé–‹å§‹æ–°å¢ä½¿ç”¨è€…ã€‚
```shell
$ mysql -uroot -p
```
```sql
mysql> INSERT INTO mysql.user(host,user,password) VALUES ('%','hive',password('hive'));
mysql> GRANT ALL ON *.* TO 'hive'@localhost IDENTIFIED BY 'hive' WITH GRANT OPTION;
mysql> FLUSH PRIVILEGES;
mysql> QUIT;
```

æ¸¬è©¦å®‰è£æ˜¯å¦æˆåŠŸ
```shell
$ mysql -uhive -phive -e "select 1+1"
```

## Sqoop

åƒè€ƒè³‡æ–™
- [Apache Sqoop å®˜ç¶²](http://sqoop.apache.org/)
- [Apache Sqoop documentation - Installation](https://sqoop.apache.org/docs/1.99.1/Installation.html)

åˆ°[Apache Download Mirrors](http://www.apache.org/dyn/closer.lua/sqoop/)ä¸‹è¼‰æœ€æ–°çš„sqoop packageï¼Œè§£å£“ç¸®å¾Œæ¬ç§»åˆ°```/usr/local```ç›®éŒ„ä¹‹ä¸‹ã€‚
```shell
$ wget http://ftp.tc.edu.tw/pub/Apache/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
$ tar zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
$ sudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha /usr/local/sqoop
```

è¨­å®šç’°å¢ƒè®Šæ•¸ï¼Œä¿®æ”¹```~/.bashrc```åŠ å…¥ä»¥ä¸‹å…§å®¹
```
export SQOOP_HOME=/usr/local/sqoop
export PATH=\$PATH:\$SQOOP_HOME/bin
```

æ­¤å¤–é‚„è¦å®‰è£mysql connector
```shell
$ wget http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.38.tar.gz
$ tar zxf mysql-connector-java-5.1.38.tar.gz
$ sudo mv mysql-connector-java-5.1.38/mysql-connector-java-5.1.38-bin.jar /usr/local/sqoop/lib/
```

ç”±æ–¼æ²’æœ‰å®‰è£HBaseã€HCatalogã€Accumuloã€Zookeeperï¼Œç¨å¾ŒåŸ·è¡Œsqoopçš„æ™‚å€™æœƒè·³å‡ºä¸€å †Warningï¼Œä¸å½±éŸ¿ä¸ç†å®ƒ

## Scala

åƒè€ƒè³‡æ–™
- [Scala å®˜ç¶²](http://www.scala-lang.org/)

åˆ°[Scala Download](http://www.scala-lang.org/download/)ä¸‹è¼‰æœ€æ–°çš„scala packageï¼Œè§£å£“ç¸®å¾Œæ¬ç§»åˆ°```/usr/local```ç›®éŒ„ä¹‹ä¸‹ã€‚
```shell
$ wget http://downloads.typesafe.com/scala/2.11.7/scala-2.11.7.tgz
$ tar zxf scala-2.11.7.tgz
$ sudo mv scala-2.11.7 /usr/local/scala
```

è¨­å®šç’°å¢ƒè®Šæ•¸ï¼Œä¿®æ”¹```~/.bashrc```åŠ å…¥ä»¥ä¸‹å…§å®¹
```
export SCALA_HOME=/usr/local/scala
export PATH=$PATH:$SCALA_HOME/bin
```

æ¸¬è©¦ Hive åŠŸèƒ½æ˜¯å¦æ­£å¸¸
```shell
$ scala -e 'println("Hello World!")'
```

## Spark

åƒè€ƒè³‡æ–™
- [Spark å®˜ç¶²](http://spark.apache.org/)

åˆ°[Spark Download](http://spark.apache.org/downloads.html)ä¸‹è¼‰æœ€æ–°çš„spark packageï¼Œè§£å£“ç¸®å¾Œæ¬ç§»åˆ°```/usr/local```ç›®éŒ„ä¹‹ä¸‹ã€‚
```shell
$ wget http://ftp.tc.edu.tw/pub/Apache/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz
$ tar zxf spark-1.5.2-bin-hadoop2.6.tgz
$ sudo mv spark-1.5.2-bin-hadoop2.6 /usr/local/spark
```

è¨­å®šç’°å¢ƒè®Šæ•¸ï¼Œä¿®æ”¹```~/.bashrc```åŠ å…¥ä»¥ä¸‹å…§å®¹
```
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$(hadoop classpath)
```

æ¸¬è©¦ Spark åŠŸèƒ½æ˜¯å¦æ­£å¸¸
```shell
$ run-example SparkPi
```

## å¤§åŠŸå‘ŠæˆğŸ˜

åˆ°æ­¤æˆ‘å€‘çš„è™›æ“¬æ©Ÿå™¨ä¸Šé¢æ¶è¨­äº†Hadoopã€Hiveã€MySQLã€Scalaã€Sparkï¼Œå¯ä»¥å¥½å¥½è¤‡ç¿’èª²å ‚ä¸Šå­¸åˆ°çš„çŸ¥è­˜ã€‚ä»¥ä¸‹ç¸½çµæ¯æ¬¡é–‹æ©Ÿ/é—œæ©Ÿã€ç™»å…¥/ç™»å‡ºéœ€è¦åŸ·è¡ŒæŒ‡ä»¤ã€‚

å¾Host OSé–‹å•Ÿè™›æ“¬æ©Ÿå™¨ï¼Œé–‹æ©Ÿå¾Œsshé€£ç·šç™»å…¥
```shell
$ cd adsctw
$ vagrant up
$ vagrant ssh
```

ç™»å…¥è™›æ“¬æ©Ÿå¾Œï¼Œåˆ‡æ›ä½¿ç”¨è€…ï¼Œä¸¦å•Ÿå‹•hadoopæœå‹™
```shell
$ sudo su - hadoop
$ start-dfs.sh
```

ç™»å‡ºç³»çµ±ã€é—œé–‰æˆ–æš«åœè™›æ“¬æ©Ÿå™¨
```shell
$ exit
$ vagrant halt
$ vagrant suspend
```

That's all! Enjoy your learning :)
